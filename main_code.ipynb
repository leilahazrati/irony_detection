{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7efd9786-ad2b-4ddb-9e0a-411518ce6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from TweetNormalizer import normalizeTweet\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import regex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "from xml.dom.minidom import parse\n",
    "from xml.etree import ElementTree\n",
    "import xmltodict\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "import demoji\n",
    "#from emoji.unicode_codes import UNICODE_EMOJI\n",
    "import emot as e\n",
    "#from emot.emo_unicode import UNICODE_EMOJI, UNICODE_EMOJI_ALIAS, EMOTICONS_EMO\n",
    "#from emoji_translate.emoji_translate import Translator\n",
    "from html import unescape\n",
    "import html\n",
    "import unicodedata\n",
    "from googletrans import Translator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "from xml.dom.minidom import parse\n",
    "from xml.etree import ElementTree\n",
    "import xmltodict\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import zipfile as zf\n",
    "from html import unescape\n",
    "import html\n",
    "import unicodedata\n",
    "import xml.etree.ElementTree\n",
    "import bs4\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from kneed import KneeLocator\n",
    "import pickle\n",
    "import patoolib\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from palm_rlhf_pytorch import PaLM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "from artlearn import FuzzyART\n",
    "import glob\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score ,recall_score , f1_score\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12f2a7-3a69-4fbf-ac96-710806b330d3",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768bb826-c5d7-4cd9-ab13-97d756736271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting pan22-author-profiling-training-2022-03-29-1.zip ...\n",
      "patool: ... pan22-author-profiling-training-2022-03-29-1.zip extracted to `pan22-author-profiling-training-2022-03-29-14' (local file exists).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pan22-author-profiling-training-2022-03-29-14'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patoolib.extract_archive(\"pan22-author-profiling-training-2022-03-29-1.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e99795-f2cc-4fac-ab14-b9fc52925342",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('./pan22-author-profiling-training-2022-03-29/en')\n",
    "files.remove('truth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24c5a6d-c7f7-4a90-b967-90d84c66c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "for file in files:\n",
    "    if not file.startswith('.'):\n",
    "        ID = 0\n",
    "        tree = ET.parse(f'./pan22-author-profiling-training-2022-03-29-1/pan22-author-profiling-training-2022-03-29/en/{file}')\n",
    "        root = tree.getroot()\n",
    "        docs = root.find('documents')\n",
    "        for doc in docs.findall('document'):\n",
    "            ID += 1\n",
    "            row = [file, ID, doc.text] # add bert\n",
    "            #print(row)\n",
    "            data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82c09ac-fec8-4cf4-95b2-4cfb22263d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['author_name', 'tweet_id', 'tweet'] \n",
    "with open('Clusterdataset.csv', 'w') as f:\n",
    "    write = csv.writer(f)      \n",
    "    write.writerow(fields)\n",
    "    write.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9873813-d50c-424a-b8b1-44a99a836c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Clusterdataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4a88c-e680-46de-85ac-b40a03f84f7b",
   "metadata": {},
   "source": [
    "# preporcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ba176c6-7b14-423c-8066-2c321d6e1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"twitterSlang/twitterSlang.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f,delimiter=',')\n",
    "    slangTwitter = {row[0]:row[1] for row in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c242cdf-17bf-484b-b057-624021e062c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceTwitterSlang(tweet):\n",
    "    \"\"\"\n",
    "    Replaces twitter specific slang, based on a dictionary created from various online sources\n",
    "    \"\"\"\n",
    "    tokens = tweet.split(' ')\n",
    "    return ' '.join([slangTwitter[token.lower()] if token.lower() in slangTwitter else token for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2978d3e2-b449-4c05-8f61-28cdbcf59d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_control_char(txt):\n",
    "\n",
    "    txt = str(txt).strip()\n",
    "    txt = \"\".join(ch for ch in txt if unicodedata.category(ch)!=\"Cc\")\n",
    "    txt = re.sub(r'[\\u200d\\u200f\\u202b\\x01]', '', txt)\n",
    "    txt = re.sub(r'(\\u200c)+', r'\\1', txt)\n",
    "    txt = re.sub(r'(\\s)+\\u200c', r'\\1', txt)\n",
    "    txt = re.sub(r'\\u200c(\\s)+', ' ', txt)\n",
    "    txt = re.sub(r'([0-9,\\.\\(\\)\\[\\];،؛,])\\u200c', r'\\1', txt)\n",
    "    txt = re.sub(r'\\u200c([0-9,\\.\\(\\)\\]\\[;،؛,])', r'\\1', txt)\n",
    "    txt = re.sub(r'^[\\s\\u200c]*(.*?)[\\s\\u200c]*$', r'\\1', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3e9e1aa-e4b2-4d50-bf94-c63a2393f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3e146bd-35e3-4c9f-b233-a67e7e6e3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_en(input_txt):\n",
    "    words = input_txt.lower().split()\n",
    "    noise_free_words = [word for word in words if word not in stop] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ddd96b-d8e2-41ff-9f65-b700eb9e7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seprate_twits(txt):\n",
    "    txt = str(txt).replace(\"|||\",\".\\n\") #todo : why?\n",
    "    txt = txt.replace(\"|\",\"\")\n",
    "    txt = re.sub(r' +', ' ', txt)\n",
    "    txt = re.sub(r' +\\.', '.', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90140ef1-375b-4a4e-ba55-93d84870d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep clean\n",
    "df[\"CLEAN_TEXT\"] = df[\"tweet\"].apply(replaceTwitterSlang)\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].apply(lambda s: str(s).lower())\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].apply(lambda s: html.unescape(str(s)))\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].apply(lambda s: remove_control_char(s))\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].apply(lambda s: re.sub(\"(\\w+://\\S+)\", \" \", s))\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].replace('#user#', '', regex=True)\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].replace('#hashtag#', '', regex=True)\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].replace('#url#', '', regex=True)\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].apply(lambda s: rem_en(s))\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].apply(lambda s: ' '.join(re.sub(\"[\\*•.,!?:;\\-\\+='...\\\"@#_%&\\$…]\", \" \", s).split()))\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].apply(lambda s: ' '.join(re.sub(\"[“”’‘]\", \"\", s).split()))\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].replace('(number\\s+)+', 'number ', regex=True)\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].replace(r' +', ' ', regex=True)\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].apply(lambda s: seprate_twits(s))\n",
    "df[\"CLEAN_TEXT\"] = df[\"CLEAN_TEXT\"].replace(r'[^a-z\\s\\.]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5da9088c-1f7d-4823-a438-cc0cc6594aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26969/2659571774.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"CLEAN_TEXT\"][i] = emoji.demojize(inputs[\"CLEAN_TEXT\"],delimiters=(\" \", \" \"))\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,419):\n",
    "    inputs = df.to_dict('records')[i]\n",
    "    df[\"CLEAN_TEXT\"][i] = emoji.demojize(inputs[\"CLEAN_TEXT\"],delimiters=(\" \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "303144f4-692b-4721-a652-645fbb3254cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    return emoji.get_emoji_regexp().sub(u'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86d40969-1f0c-47af-9b75-6de42e39fa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26969/3148388468.py:2: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  return emoji.get_emoji_regexp().sub(u'', string)\n"
     ]
    }
   ],
   "source": [
    "# lite cleen\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"tweet\"].apply(replaceTwitterSlang)\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].apply(lambda s: str(s).lower())\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].apply(lambda s: html.unescape(str(s)))\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].apply(lambda s: remove_control_char(s))\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].apply(lambda s: re.sub(\"(\\w+://\\S+)\", \" \", s))\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].replace('#user#', '', regex=True)\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].replace('#hashtag#', '', regex=True)\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].replace('#url#', '', regex=True)\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].apply(lambda s: ' '.join(re.sub(\"[\\*•:;\\-\\+=\\\"@#_%&\\$…]\", \" \", s).split()))\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].apply(lambda s: ' '.join(re.sub(\"[“”]\", \"\", s).split()))\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].apply(lambda s: re.sub(\"[’‘]\", \"'\", s))\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].replace(r' +', ' ', regex=True)\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].apply(lambda s: seprate_twits(s))\n",
    "df[\"LITE_CLEAN_TEXT\"] = df[\"LITE_CLEAN_TEXT\"].apply(lambda s: remove_emoji(s)) #todo: remove only emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fdbac0-b514-4a48-a60b-950f93647580",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f50f6fd5-2c70-4929-ba57-e9cdbd1a7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict()\n",
    "with open('./truth.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in lines:\n",
    "        name_label = i.split(':::')\n",
    "        # lab = name_label[1].split('\\')\n",
    "        res[name_label[0]] = name_label[1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c76b7d2-58e7-4651-b474-27c574f81407",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list()\n",
    "for i in list(df['author_name']):\n",
    "    lab = res[i[:-4]]\n",
    "    if lab == 'I':       \n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cfc102a-18d9-4006-af43-2dfa9fa44ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73024796-068e-43ff-ba9c-7048ccd5d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Cluster_clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfb9c5-ab39-47bc-8779-02afd77653fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Cluster_clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65ac9313-c4d2-4119-aff9-fa4c6edd9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.iterrows():\n",
    "     if type(i[1][4]) == float:\n",
    "            df =df.drop(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4caa86-b7a6-411f-a47e-fe920c39b4ba",
   "metadata": {},
   "source": [
    "# Feature Extraction with Bertweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caa791c6-8da5-4d99-ace5-81850426ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24fe7199-5bc7-417b-9ca2-dbe60768c54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\",return_dict=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4351e3ec-befb-4bca-8249-4d46af291f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dim_reduction(sentences, tokenizer, model):\n",
    "    '''\n",
    "        This method will accept array of sentences, roberta tokenizer & model\n",
    "        next it will call methods for dimention reduction\n",
    "    '''\n",
    "    vecs = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "                \n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True,  max_length=64)\n",
    "            inputs['input_ids'] = inputs['input_ids']\n",
    "            inputs['attention_mask'] = inputs['attention_mask']\n",
    "\n",
    "            hidden_states = model(**inputs, return_dict=True, output_hidden_states=True).hidden_states\n",
    "\n",
    "            #Averaging the first & last hidden states\n",
    "            output_hidden_state = (hidden_states[-1] + hidden_states[1]).mean(dim=1)\n",
    "\n",
    "            vec = output_hidden_state.cpu().numpy()[0]\n",
    "\n",
    "            vecs.append(vec)\n",
    "\n",
    "\n",
    "    #Finding Kernal\n",
    "    kernel, bias = compute_kernel_bias([vecs])\n",
    "    kernel = kernel[:, :128]\n",
    "    embeddings = []\n",
    "    embeddings = np.vstack(vecs)\n",
    "    embeddings = transform_and_normalize(embeddings, \n",
    "                kernel=kernel,\n",
    "                bias=bias\n",
    "            )\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b141f233-84a8-49aa-b48a-5a544f74da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_normalize(vecs, kernel, bias):\n",
    "    \"\"\"\n",
    "        Applying transformation then standardize\n",
    "    \"\"\"\n",
    "    if not (kernel is None or bias is None):\n",
    "        vecs = (vecs + bias).dot(kernel)\n",
    "    return normalize(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44a6398c-2e5c-4388-b997-aedeba722918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vecs):\n",
    "    \"\"\"\n",
    "        Standardization\n",
    "    \"\"\"\n",
    "    return vecs / (vecs**2).sum(axis=1, keepdims=True)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f317ecdb-1cb8-4771-82dc-76c1a6e0032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel_bias(vecs):\n",
    "    \"\"\"\n",
    "    Calculate Kernal & Bias for the final transformation - y = (x + bias).dot(kernel)\n",
    "    \"\"\"\n",
    "    vecs = np.concatenate(vecs, axis=0)\n",
    "    mu = vecs.mean(axis=0, keepdims=True)\n",
    "    cov = np.cov(vecs.T)\n",
    "    u, s, vh = np.linalg.svd(cov)\n",
    "    W = np.dot(u, np.diag(s**0.5))\n",
    "    W = np.linalg.inv(W.T)\n",
    "    return W, -mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fa8d2ce-efe7-44b3-8ca3-99b56134e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dimention_Reduction(new_df):\n",
    "    embeddings = Dim_reduction(list(new_df['CLEAN_TEXT']), tokenizer, bertweet)\n",
    "    newembedding = list()\n",
    "    for i in range(len(embeddings)):\n",
    "        newembedding.append(embeddings[i])\n",
    "    new_df['DM_feature'] = newembedding\n",
    "    return newembedding , new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f956e2-dcfc-469d-acf8-e2ae7d74bd4f",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b91017d-c086-445e-a607-61bed20d6a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_each_author(newembedding , new_df):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(newembedding)\n",
    "    nbrs = NearestNeighbors(n_neighbors = 5).fit(X_scaled) \n",
    "    neigh_dist, neigh_ind = nbrs.kneighbors(X_scaled)\n",
    "    sort_neigh_dist = np.sort(neigh_dist, axis = 0) \n",
    "    k_dist = sort_neigh_dist[:, 4]\n",
    "    kneedle = KneeLocator(x = range(1, len(neigh_dist)+1), y = k_dist, S = 1.0, \n",
    "                          curve = \"concave\", direction = \"increasing\", online=True)\n",
    "    clusters = DBSCAN(eps = float(str(kneedle.knee_y)[:3]), min_samples = 3 ).fit(newembedding)\n",
    "    new_df['cluster_label'] = clusters.labels_\n",
    "    noise_list = list()\n",
    "    for i in range(len(clusters.labels_)):\n",
    "        if clusters.labels_[i] == -1 :\n",
    "            noise_list.append(1)\n",
    "        else:\n",
    "            noise_list.append(0)\n",
    "    new_df['noise'] = noise_list\n",
    "    new__df = knn_removing_noise(new_df)\n",
    "    return  new__df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b75e28d-c6ba-4d93-9f7b-71fb82640d32",
   "metadata": {},
   "source": [
    "# Removing noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb273984-872d-472f-8782-5dfc48dcd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_removing_noise(new_df):\n",
    "    selected_columns = new_df[[\"DM_feature\",\"cluster_label\"]]\n",
    "    without_noise_df = selected_columns.copy()\n",
    "    test_set = without_noise_df[without_noise_df['cluster_label'] == -1]\n",
    "    if (len(test_set) > 0 ):\n",
    "        train_set = without_noise_df[without_noise_df['cluster_label'] != -1]\n",
    "        x_train = train_set['DM_feature']\n",
    "        y_train = train_set['cluster_label']\n",
    "        x_test = test_set['DM_feature']\n",
    "        y_test = test_set['cluster_label']\n",
    "        train_features = x_train.apply(pd.Series)\n",
    "        train_processed = pd.concat([x_train, train_features], axis=1)\n",
    "        train_processed = train_processed.drop('DM_feature', axis=1)\n",
    "        knn_classifier = KNeighborsClassifier(n_neighbors = 3 )\n",
    "        knn_classifier.fit(train_processed, y_train)\n",
    "        test_features = x_test.apply(pd.Series)\n",
    "        test_processed = pd.concat([x_test, test_features], axis=1)\n",
    "        test_processed = test_processed.drop('DM_feature', axis=1)\n",
    "        predicted_labels = knn_classifier.predict(test_processed)\n",
    "        predicted_labels = pd.DataFrame({'predicted_label':predicted_labels})\n",
    "        j = 0\n",
    "        for i in range(len(new_df)):\n",
    "            if (new_df['cluster_label'].iloc[i] == -1):\n",
    "                new_df['cluster_label'].iloc[i] = predicted_labels.iloc[j] \n",
    "                j = j + 1 \n",
    "        return new_df #todo: add one clumn to sepreate knn from clustering\n",
    "    else:\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f952dc-64e8-4ceb-b89a-e8c271844941",
   "metadata": {},
   "source": [
    "# Feature Extraction with PaLm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db8190f8-f6f5-4838-b720-7b340401cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f25ccf7-cbf5-4edb-a262-671049f5b749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PaLM(\n",
    "    num_tokens=50304, dim=1024, depth=24, dim_head=128, heads=8, flash_attn=False, qk_rmsnorm = False,\n",
    ").to(device).eval()\n",
    "checkpoint = torch.load('./PaLM/checkpoints/palm_410m_8k_v0.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b84c9b2-c78e-4c99-bded-cb74817e6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_name_list = df['author_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec7f5606-e487-40ed-88e7-f40a886dd59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "for i in list(df['author_name'].unique()):\n",
    "    try:\n",
    "        new_df = df[df['author_name'] == i ]\n",
    "        newembedding,new__df = Dimention_Reduction(new_df)\n",
    "        df_author = clustering_each_author(newembedding, new__df)\n",
    "      #  with open(f'./author_clusterrr/{i[:-4]}.pk', 'wb') as handle:\n",
    "      #      pickle.dump(text_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(f'./Final_dimention_128/{i[:-4]}.pk', 'wb') as handle:\n",
    "            pickle.dump(df_author, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    except:\n",
    "        print(i)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9fc456-6dcd-44e4-9403-883613ff4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, i in enumerate(author_name_list[:]):\n",
    "    with open(f'./Final_dimention_128/{i[:-4]}.pk', 'rb') as handle:\n",
    "        mini_df = pickle.load(handle)\n",
    "        palm_features = list()\n",
    "        res = Counter(list(mini_df['cluster_label']))\n",
    "    for j in range (len (mini_df)):\n",
    "        encoded_input = tokenizer (mini_df['LITE_CLEAN_TEXT'].iloc[j] , return_tensors='pt')\n",
    "        features = model(x = encoded_input['input_ids'], return_only_embedding=True)\n",
    "        features = features.detach().numpy()\n",
    "        palm_features.append(features)\n",
    "    mini_df['palm_features'] = palm_features\n",
    "    del palm_features\n",
    "    with open(f'./author_dffff_palm/{i[:-4]}.pk', 'wb') as handle:\n",
    "        pickle.dump(mini_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0de31b52-736b-4cfe-a21c-f974272dbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = list()\n",
    "pickle_files = glob.glob ('author_dffff_palm/*.pk')\n",
    "merged_df = pd.DataFrame()\n",
    "for pickle_file in pickle_files[:]:\n",
    "    final_df = pd.read_pickle(pickle_file)\n",
    "    all_data.append(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7548467c-f168-484f-b0f7-87aacf88fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(all_data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "284e5b90-9a29-40a9-b791-9b385313f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_list = list()\n",
    "for feature in (final_df['palm_features']):\n",
    "    f = torch.from_numpy(feature)\n",
    "  #  print(type(feature))\n",
    "    means_list.append(torch.mean(f , 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a010c25d-f650-4e31-85c5-73e8be36e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['mean_palm'] = means_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c0b72d0-4089-4af8-addd-191ebc6b8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size = final_df.groupby (['author_name','cluster_label']).size().reset_index(name = 'cluster_size')\n",
    "final_df= pd.merge(final_df,cluster_size , on = ['author_name','cluster_label'], how = 'left')\n",
    "final_df['mean_palm'] = final_df['mean_palm'].apply(lambda x: x.numpy())\n",
    "feature_aggregates = final_df.groupby(['author_name','cluster_label']).agg( feature_avg = ('mean_palm', 'mean')).reset_index()\n",
    "final_df = pd.merge(final_df, feature_aggregates , on = ['author_name','cluster_label'], how = 'left')\n",
    "final_df = final_df.drop(columns = ['tweet_id'])\n",
    "final_df = final_df.drop_duplicates(subset=['author_name','cluster_label'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ccd0953-f970-4a23-be87-be0a612efb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorlist = df['author_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3aea3091-efe4-4f41-aad2-04d6b991ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_list = list()\n",
    "for i in authorlist:\n",
    "    a_df = final_df[final_df.author_name == i]\n",
    "    sum_size = 0\n",
    "    summation = 0\n",
    "    for j in list(a_df.iterrows()):\n",
    "        summation += j[1]['feature_avg'] * j[1]['cluster_size']\n",
    "        sum_size += j[1]['cluster_size']\n",
    "    avg = summation / sum_size\n",
    "    avg_list.append([i, avg, j[1]['label']])\n",
    "avg_df = pd.DataFrame(avg_list , columns = ['author' , 'feature' , 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4aa9c75b-5330-4e77-99f3-b03b1f6b5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_feature = list()\n",
    "for i in list(avg_df['feature']):\n",
    "    np_feature.append(i.reshape(-1))\n",
    "avg_df['np_feature'] =  np_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc0f90cf-096b-4978-a2fb-a9902d5e281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splite train and test data\n",
    "X = avg_df['np_feature'] \n",
    "X = np.array([np.asarray(i).astype('float32') for i in X])\n",
    "y = avg_df['label']\n",
    "#y = to_categorical(y) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "280b93cd-2142-4d9a-924f-4936c8fe64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4e04290-a5ab-4943-86e2-e3c64bb44396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "11/11 [==============================] - 4s 96ms/step - loss: 17.3713 - accuracy: 0.5149 - val_loss: 17.3203 - val_accuracy: 0.5357\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 17.2457 - accuracy: 0.6369 - val_loss: 17.3162 - val_accuracy: 0.5238\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 17.1795 - accuracy: 0.6935 - val_loss: 17.3115 - val_accuracy: 0.5238\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 17.0882 - accuracy: 0.7917 - val_loss: 17.3008 - val_accuracy: 0.5238\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 17.0421 - accuracy: 0.8274 - val_loss: 17.2913 - val_accuracy: 0.5357\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 17.0085 - accuracy: 0.8542 - val_loss: 17.2747 - val_accuracy: 0.5238\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 1s 82ms/step - loss: 16.9587 - accuracy: 0.8869 - val_loss: 17.2544 - val_accuracy: 0.5476\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 1s 64ms/step - loss: 16.9213 - accuracy: 0.8988 - val_loss: 17.2376 - val_accuracy: 0.6071\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 16.8928 - accuracy: 0.9315 - val_loss: 17.2222 - val_accuracy: 0.6548\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 1s 95ms/step - loss: 16.8770 - accuracy: 0.9286 - val_loss: 17.2047 - val_accuracy: 0.6667\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 16.8544 - accuracy: 0.9494 - val_loss: 17.1891 - val_accuracy: 0.7024\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 1s 74ms/step - loss: 16.8175 - accuracy: 0.9702 - val_loss: 17.1724 - val_accuracy: 0.7262\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.7941 - accuracy: 0.9643 - val_loss: 17.1525 - val_accuracy: 0.7262\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 1s 67ms/step - loss: 16.7901 - accuracy: 0.9435 - val_loss: 17.1292 - val_accuracy: 0.7619\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 1s 61ms/step - loss: 16.7720 - accuracy: 0.9554 - val_loss: 17.1057 - val_accuracy: 0.7976\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 1s 97ms/step - loss: 16.7465 - accuracy: 0.9732 - val_loss: 17.0786 - val_accuracy: 0.8095\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 1s 89ms/step - loss: 16.7318 - accuracy: 0.9851 - val_loss: 17.0506 - val_accuracy: 0.8333\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 1s 91ms/step - loss: 16.7081 - accuracy: 0.9792 - val_loss: 17.0205 - val_accuracy: 0.8690\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 16.6961 - accuracy: 0.9673 - val_loss: 16.9913 - val_accuracy: 0.8929\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 1s 142ms/step - loss: 16.6824 - accuracy: 0.9613 - val_loss: 16.9592 - val_accuracy: 0.9048\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 16.6631 - accuracy: 0.9762 - val_loss: 16.9268 - val_accuracy: 0.8929\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 16.6527 - accuracy: 0.9792 - val_loss: 16.8946 - val_accuracy: 0.9048\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.6276 - accuracy: 0.9762 - val_loss: 16.8656 - val_accuracy: 0.9048\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.6224 - accuracy: 0.9792 - val_loss: 16.8374 - val_accuracy: 0.9048\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 16.6074 - accuracy: 0.9792 - val_loss: 16.8109 - val_accuracy: 0.9048\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 16.6098 - accuracy: 0.9762 - val_loss: 16.7852 - val_accuracy: 0.9167\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.5703 - accuracy: 0.9821 - val_loss: 16.7620 - val_accuracy: 0.9048\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.5534 - accuracy: 0.9851 - val_loss: 16.7372 - val_accuracy: 0.9167\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.5507 - accuracy: 0.9851 - val_loss: 16.7160 - val_accuracy: 0.9167\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 1s 63ms/step - loss: 16.5345 - accuracy: 0.9881 - val_loss: 16.6952 - val_accuracy: 0.9524\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 16.5133 - accuracy: 0.9821 - val_loss: 16.6762 - val_accuracy: 0.9405\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.4946 - accuracy: 0.9851 - val_loss: 16.6556 - val_accuracy: 0.9405\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.4968 - accuracy: 0.9851 - val_loss: 16.6345 - val_accuracy: 0.9405\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.4804 - accuracy: 0.9792 - val_loss: 16.6137 - val_accuracy: 0.9405\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 16.4487 - accuracy: 0.9911 - val_loss: 16.5938 - val_accuracy: 0.9524\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.4458 - accuracy: 0.9881 - val_loss: 16.5755 - val_accuracy: 0.9524\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.4239 - accuracy: 0.9881 - val_loss: 16.5558 - val_accuracy: 0.9524\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 16.4272 - accuracy: 0.9821 - val_loss: 16.5370 - val_accuracy: 0.9524\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.3994 - accuracy: 0.9851 - val_loss: 16.5222 - val_accuracy: 0.9524\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.3808 - accuracy: 0.9881 - val_loss: 16.5067 - val_accuracy: 0.9524\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.3761 - accuracy: 0.9881 - val_loss: 16.4902 - val_accuracy: 0.9524\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.3465 - accuracy: 0.9911 - val_loss: 16.4742 - val_accuracy: 0.9524\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 16.3427 - accuracy: 0.9851 - val_loss: 16.4587 - val_accuracy: 0.9524\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 16.3389 - accuracy: 0.9851 - val_loss: 16.4426 - val_accuracy: 0.9524\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.3146 - accuracy: 0.9881 - val_loss: 16.4297 - val_accuracy: 0.9524\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.2982 - accuracy: 0.9911 - val_loss: 16.4162 - val_accuracy: 0.9524\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.2851 - accuracy: 0.9881 - val_loss: 16.4027 - val_accuracy: 0.9286\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 16.2714 - accuracy: 0.9911 - val_loss: 16.3886 - val_accuracy: 0.9405\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.2710 - accuracy: 0.9851 - val_loss: 16.3729 - val_accuracy: 0.9405\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.2326 - accuracy: 0.9911 - val_loss: 16.3574 - val_accuracy: 0.9405\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.2146 - accuracy: 0.9911 - val_loss: 16.3419 - val_accuracy: 0.9405\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 16.2197 - accuracy: 0.9881 - val_loss: 16.3256 - val_accuracy: 0.9405\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.2004 - accuracy: 0.9911 - val_loss: 16.3100 - val_accuracy: 0.9286\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.1727 - accuracy: 0.9940 - val_loss: 16.2996 - val_accuracy: 0.9286\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.1607 - accuracy: 0.9940 - val_loss: 16.2858 - val_accuracy: 0.9286\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.1433 - accuracy: 0.9911 - val_loss: 16.2701 - val_accuracy: 0.9286\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.1459 - accuracy: 0.9881 - val_loss: 16.2511 - val_accuracy: 0.9405\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 16.1210 - accuracy: 0.9970 - val_loss: 16.2344 - val_accuracy: 0.9405\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 16.0938 - accuracy: 0.9940 - val_loss: 16.2183 - val_accuracy: 0.9286\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.0752 - accuracy: 0.9911 - val_loss: 16.2022 - val_accuracy: 0.9286\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 16.0775 - accuracy: 0.9881 - val_loss: 16.1878 - val_accuracy: 0.9286\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 16.0631 - accuracy: 0.9940 - val_loss: 16.1736 - val_accuracy: 0.9286\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.0352 - accuracy: 0.9970 - val_loss: 16.1567 - val_accuracy: 0.9286\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 16.0196 - accuracy: 0.9940 - val_loss: 16.1404 - val_accuracy: 0.9286\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 16.0040 - accuracy: 1.0000 - val_loss: 16.1249 - val_accuracy: 0.9286\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.9884 - accuracy: 0.9970 - val_loss: 16.1082 - val_accuracy: 0.9286\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.9799 - accuracy: 0.9881 - val_loss: 16.0908 - val_accuracy: 0.9405\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.9516 - accuracy: 0.9970 - val_loss: 16.0734 - val_accuracy: 0.9405\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.9457 - accuracy: 0.9940 - val_loss: 16.0553 - val_accuracy: 0.9405\n",
      "Epoch 70/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.9161 - accuracy: 1.0000 - val_loss: 16.0395 - val_accuracy: 0.9405\n",
      "Epoch 71/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.9049 - accuracy: 0.9940 - val_loss: 16.0238 - val_accuracy: 0.9405\n",
      "Epoch 72/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.8892 - accuracy: 1.0000 - val_loss: 16.0082 - val_accuracy: 0.9286\n",
      "Epoch 73/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 15.8734 - accuracy: 0.9970 - val_loss: 15.9932 - val_accuracy: 0.9286\n",
      "Epoch 74/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.8565 - accuracy: 0.9970 - val_loss: 15.9761 - val_accuracy: 0.9286\n",
      "Epoch 75/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 15.8592 - accuracy: 0.9881 - val_loss: 15.9600 - val_accuracy: 0.9286\n",
      "Epoch 76/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.8218 - accuracy: 0.9970 - val_loss: 15.9428 - val_accuracy: 0.9286\n",
      "Epoch 77/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.8016 - accuracy: 0.9970 - val_loss: 15.9242 - val_accuracy: 0.9286\n",
      "Epoch 78/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 15.7906 - accuracy: 0.9970 - val_loss: 15.9067 - val_accuracy: 0.9286\n",
      "Epoch 79/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.7767 - accuracy: 0.9970 - val_loss: 15.8893 - val_accuracy: 0.9286\n",
      "Epoch 80/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.7633 - accuracy: 0.9970 - val_loss: 15.8700 - val_accuracy: 0.9286\n",
      "Epoch 81/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.7374 - accuracy: 1.0000 - val_loss: 15.8527 - val_accuracy: 0.9286\n",
      "Epoch 82/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.7334 - accuracy: 1.0000 - val_loss: 15.8335 - val_accuracy: 0.9286\n",
      "Epoch 83/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.7048 - accuracy: 1.0000 - val_loss: 15.8167 - val_accuracy: 0.9405\n",
      "Epoch 84/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.6980 - accuracy: 1.0000 - val_loss: 15.7989 - val_accuracy: 0.9405\n",
      "Epoch 85/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.6695 - accuracy: 1.0000 - val_loss: 15.7808 - val_accuracy: 0.9405\n",
      "Epoch 86/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.6496 - accuracy: 0.9970 - val_loss: 15.7637 - val_accuracy: 0.9405\n",
      "Epoch 87/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.6321 - accuracy: 1.0000 - val_loss: 15.7511 - val_accuracy: 0.9405\n",
      "Epoch 88/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.6189 - accuracy: 0.9970 - val_loss: 15.7352 - val_accuracy: 0.9405\n",
      "Epoch 89/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.6007 - accuracy: 1.0000 - val_loss: 15.7186 - val_accuracy: 0.9405\n",
      "Epoch 90/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.5770 - accuracy: 0.9970 - val_loss: 15.7015 - val_accuracy: 0.9405\n",
      "Epoch 91/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.5651 - accuracy: 1.0000 - val_loss: 15.6842 - val_accuracy: 0.9405\n",
      "Epoch 92/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 15.5403 - accuracy: 1.0000 - val_loss: 15.6640 - val_accuracy: 0.9405\n",
      "Epoch 93/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.5286 - accuracy: 0.9970 - val_loss: 15.6439 - val_accuracy: 0.9405\n",
      "Epoch 94/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.5112 - accuracy: 1.0000 - val_loss: 15.6262 - val_accuracy: 0.9405\n",
      "Epoch 95/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.4848 - accuracy: 1.0000 - val_loss: 15.6071 - val_accuracy: 0.9405\n",
      "Epoch 96/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 15.4722 - accuracy: 1.0000 - val_loss: 15.5886 - val_accuracy: 0.9405\n",
      "Epoch 97/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 15.4573 - accuracy: 1.0000 - val_loss: 15.5704 - val_accuracy: 0.9405\n",
      "Epoch 98/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.4414 - accuracy: 0.9970 - val_loss: 15.5540 - val_accuracy: 0.9405\n",
      "Epoch 99/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.4188 - accuracy: 1.0000 - val_loss: 15.5365 - val_accuracy: 0.9405\n",
      "Epoch 100/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.4045 - accuracy: 0.9940 - val_loss: 15.5171 - val_accuracy: 0.9405\n",
      "Epoch 101/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 15.3896 - accuracy: 1.0000 - val_loss: 15.4999 - val_accuracy: 0.9524\n",
      "Epoch 102/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 15.3689 - accuracy: 1.0000 - val_loss: 15.4835 - val_accuracy: 0.9524\n",
      "Epoch 103/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.3499 - accuracy: 0.9970 - val_loss: 15.4651 - val_accuracy: 0.9524\n",
      "Epoch 104/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.3246 - accuracy: 1.0000 - val_loss: 15.4508 - val_accuracy: 0.9405\n",
      "Epoch 105/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 15.3065 - accuracy: 1.0000 - val_loss: 15.4331 - val_accuracy: 0.9405\n",
      "Epoch 106/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.2885 - accuracy: 1.0000 - val_loss: 15.4135 - val_accuracy: 0.9405\n",
      "Epoch 107/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.2763 - accuracy: 1.0000 - val_loss: 15.3939 - val_accuracy: 0.9405\n",
      "Epoch 108/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.2457 - accuracy: 1.0000 - val_loss: 15.3719 - val_accuracy: 0.9405\n",
      "Epoch 109/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 15.2284 - accuracy: 1.0000 - val_loss: 15.3492 - val_accuracy: 0.9405\n",
      "Epoch 110/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 15.2063 - accuracy: 1.0000 - val_loss: 15.3274 - val_accuracy: 0.9405\n",
      "Epoch 111/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.1944 - accuracy: 1.0000 - val_loss: 15.3073 - val_accuracy: 0.9405\n",
      "Epoch 112/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.1706 - accuracy: 1.0000 - val_loss: 15.2892 - val_accuracy: 0.9405\n",
      "Epoch 113/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 15.1483 - accuracy: 1.0000 - val_loss: 15.2696 - val_accuracy: 0.9405\n",
      "Epoch 114/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 15.1381 - accuracy: 1.0000 - val_loss: 15.2501 - val_accuracy: 0.9405\n",
      "Epoch 115/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.1070 - accuracy: 1.0000 - val_loss: 15.2314 - val_accuracy: 0.9405\n",
      "Epoch 116/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 15.0895 - accuracy: 1.0000 - val_loss: 15.2117 - val_accuracy: 0.9405\n",
      "Epoch 117/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.0662 - accuracy: 1.0000 - val_loss: 15.1931 - val_accuracy: 0.9405\n",
      "Epoch 118/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 15.0580 - accuracy: 1.0000 - val_loss: 15.1741 - val_accuracy: 0.9405\n",
      "Epoch 119/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 15.0312 - accuracy: 1.0000 - val_loss: 15.1538 - val_accuracy: 0.9405\n",
      "Epoch 120/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 15.0090 - accuracy: 1.0000 - val_loss: 15.1344 - val_accuracy: 0.9405\n",
      "Epoch 121/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.9929 - accuracy: 1.0000 - val_loss: 15.1143 - val_accuracy: 0.9405\n",
      "Epoch 122/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.9823 - accuracy: 0.9970 - val_loss: 15.0958 - val_accuracy: 0.9405\n",
      "Epoch 123/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 14.9503 - accuracy: 1.0000 - val_loss: 15.0769 - val_accuracy: 0.9405\n",
      "Epoch 124/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 14.9344 - accuracy: 1.0000 - val_loss: 15.0561 - val_accuracy: 0.9405\n",
      "Epoch 125/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.9090 - accuracy: 1.0000 - val_loss: 15.0346 - val_accuracy: 0.9405\n",
      "Epoch 126/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.8902 - accuracy: 1.0000 - val_loss: 15.0132 - val_accuracy: 0.9405\n",
      "Epoch 127/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.8702 - accuracy: 1.0000 - val_loss: 14.9923 - val_accuracy: 0.9405\n",
      "Epoch 128/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.8508 - accuracy: 1.0000 - val_loss: 14.9696 - val_accuracy: 0.9405\n",
      "Epoch 129/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.8257 - accuracy: 1.0000 - val_loss: 14.9480 - val_accuracy: 0.9405\n",
      "Epoch 130/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.8166 - accuracy: 0.9970 - val_loss: 14.9257 - val_accuracy: 0.9405\n",
      "Epoch 131/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.7950 - accuracy: 0.9911 - val_loss: 14.9042 - val_accuracy: 0.9405\n",
      "Epoch 132/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 14.7758 - accuracy: 0.9970 - val_loss: 14.8822 - val_accuracy: 0.9286\n",
      "Epoch 133/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.7526 - accuracy: 1.0000 - val_loss: 14.8618 - val_accuracy: 0.9405\n",
      "Epoch 134/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.7277 - accuracy: 0.9970 - val_loss: 14.8444 - val_accuracy: 0.9405\n",
      "Epoch 135/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.7101 - accuracy: 1.0000 - val_loss: 14.8281 - val_accuracy: 0.9405\n",
      "Epoch 136/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.6901 - accuracy: 1.0000 - val_loss: 14.8071 - val_accuracy: 0.9405\n",
      "Epoch 137/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.6627 - accuracy: 1.0000 - val_loss: 14.7853 - val_accuracy: 0.9405\n",
      "Epoch 138/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.6474 - accuracy: 1.0000 - val_loss: 14.7625 - val_accuracy: 0.9405\n",
      "Epoch 139/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.6255 - accuracy: 1.0000 - val_loss: 14.7412 - val_accuracy: 0.9405\n",
      "Epoch 140/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.6020 - accuracy: 1.0000 - val_loss: 14.7177 - val_accuracy: 0.9405\n",
      "Epoch 141/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.5715 - accuracy: 1.0000 - val_loss: 14.6958 - val_accuracy: 0.9524\n",
      "Epoch 142/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.5528 - accuracy: 1.0000 - val_loss: 14.6754 - val_accuracy: 0.9524\n",
      "Epoch 143/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.5512 - accuracy: 0.9970 - val_loss: 14.6557 - val_accuracy: 0.9524\n",
      "Epoch 144/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 14.5156 - accuracy: 1.0000 - val_loss: 14.6358 - val_accuracy: 0.9524\n",
      "Epoch 145/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 14.4965 - accuracy: 0.9970 - val_loss: 14.6125 - val_accuracy: 0.9524\n",
      "Epoch 146/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 14.4719 - accuracy: 1.0000 - val_loss: 14.5910 - val_accuracy: 0.9524\n",
      "Epoch 147/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.4468 - accuracy: 1.0000 - val_loss: 14.5679 - val_accuracy: 0.9524\n",
      "Epoch 148/300\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 14.4321 - accuracy: 1.0000 - val_loss: 14.5442 - val_accuracy: 0.9643\n",
      "Epoch 149/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.4062 - accuracy: 1.0000 - val_loss: 14.5201 - val_accuracy: 0.9643\n",
      "Epoch 150/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 14.3862 - accuracy: 1.0000 - val_loss: 14.4979 - val_accuracy: 0.9643\n",
      "Epoch 151/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.3571 - accuracy: 1.0000 - val_loss: 14.4750 - val_accuracy: 0.9643\n",
      "Epoch 152/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.3461 - accuracy: 1.0000 - val_loss: 14.4513 - val_accuracy: 0.9524\n",
      "Epoch 153/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 14.3194 - accuracy: 1.0000 - val_loss: 14.4293 - val_accuracy: 0.9524\n",
      "Epoch 154/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.2970 - accuracy: 1.0000 - val_loss: 14.4072 - val_accuracy: 0.9524\n",
      "Epoch 155/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.2686 - accuracy: 1.0000 - val_loss: 14.3854 - val_accuracy: 0.9643\n",
      "Epoch 156/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.2495 - accuracy: 1.0000 - val_loss: 14.3640 - val_accuracy: 0.9524\n",
      "Epoch 157/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.2256 - accuracy: 1.0000 - val_loss: 14.3425 - val_accuracy: 0.9524\n",
      "Epoch 158/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 14.2093 - accuracy: 1.0000 - val_loss: 14.3210 - val_accuracy: 0.9524\n",
      "Epoch 159/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 14.1894 - accuracy: 0.9940 - val_loss: 14.3004 - val_accuracy: 0.9524\n",
      "Epoch 160/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.1575 - accuracy: 1.0000 - val_loss: 14.2822 - val_accuracy: 0.9524\n",
      "Epoch 161/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 14.1430 - accuracy: 1.0000 - val_loss: 14.2592 - val_accuracy: 0.9524\n",
      "Epoch 162/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.1146 - accuracy: 1.0000 - val_loss: 14.2346 - val_accuracy: 0.9524\n",
      "Epoch 163/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 14.0949 - accuracy: 1.0000 - val_loss: 14.2108 - val_accuracy: 0.9524\n",
      "Epoch 164/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 14.0763 - accuracy: 1.0000 - val_loss: 14.1865 - val_accuracy: 0.9524\n",
      "Epoch 165/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 14.0542 - accuracy: 1.0000 - val_loss: 14.1623 - val_accuracy: 0.9524\n",
      "Epoch 166/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 14.0236 - accuracy: 0.9970 - val_loss: 14.1391 - val_accuracy: 0.9524\n",
      "Epoch 167/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 14.0070 - accuracy: 1.0000 - val_loss: 14.1207 - val_accuracy: 0.9405\n",
      "Epoch 168/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.9799 - accuracy: 1.0000 - val_loss: 14.0980 - val_accuracy: 0.9405\n",
      "Epoch 169/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 13.9518 - accuracy: 1.0000 - val_loss: 14.0735 - val_accuracy: 0.9524\n",
      "Epoch 170/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 13.9335 - accuracy: 1.0000 - val_loss: 14.0478 - val_accuracy: 0.9524\n",
      "Epoch 171/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 13.9130 - accuracy: 1.0000 - val_loss: 14.0228 - val_accuracy: 0.9524\n",
      "Epoch 172/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.8870 - accuracy: 1.0000 - val_loss: 13.9989 - val_accuracy: 0.9643\n",
      "Epoch 173/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.8677 - accuracy: 1.0000 - val_loss: 13.9754 - val_accuracy: 0.9643\n",
      "Epoch 174/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 13.8399 - accuracy: 1.0000 - val_loss: 13.9515 - val_accuracy: 0.9643\n",
      "Epoch 175/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.8178 - accuracy: 1.0000 - val_loss: 13.9285 - val_accuracy: 0.9643\n",
      "Epoch 176/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.7893 - accuracy: 1.0000 - val_loss: 13.9054 - val_accuracy: 0.9643\n",
      "Epoch 177/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 13.7727 - accuracy: 1.0000 - val_loss: 13.8807 - val_accuracy: 0.9643\n",
      "Epoch 178/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.7445 - accuracy: 1.0000 - val_loss: 13.8566 - val_accuracy: 0.9524\n",
      "Epoch 179/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.7246 - accuracy: 0.9970 - val_loss: 13.8339 - val_accuracy: 0.9524\n",
      "Epoch 180/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.6979 - accuracy: 1.0000 - val_loss: 13.8104 - val_accuracy: 0.9524\n",
      "Epoch 181/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.6687 - accuracy: 1.0000 - val_loss: 13.7868 - val_accuracy: 0.9524\n",
      "Epoch 182/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 13.6548 - accuracy: 1.0000 - val_loss: 13.7626 - val_accuracy: 0.9524\n",
      "Epoch 183/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 13.6260 - accuracy: 1.0000 - val_loss: 13.7364 - val_accuracy: 0.9524\n",
      "Epoch 184/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.6012 - accuracy: 1.0000 - val_loss: 13.7129 - val_accuracy: 0.9524\n",
      "Epoch 185/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 13.5792 - accuracy: 1.0000 - val_loss: 13.6893 - val_accuracy: 0.9524\n",
      "Epoch 186/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 13.5492 - accuracy: 1.0000 - val_loss: 13.6650 - val_accuracy: 0.9524\n",
      "Epoch 187/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.5308 - accuracy: 1.0000 - val_loss: 13.6403 - val_accuracy: 0.9524\n",
      "Epoch 188/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 13.5034 - accuracy: 1.0000 - val_loss: 13.6163 - val_accuracy: 0.9524\n",
      "Epoch 189/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.4825 - accuracy: 1.0000 - val_loss: 13.5924 - val_accuracy: 0.9524\n",
      "Epoch 190/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 13.4605 - accuracy: 1.0000 - val_loss: 13.5672 - val_accuracy: 0.9524\n",
      "Epoch 191/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.4327 - accuracy: 1.0000 - val_loss: 13.5441 - val_accuracy: 0.9524\n",
      "Epoch 192/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.4057 - accuracy: 1.0000 - val_loss: 13.5203 - val_accuracy: 0.9524\n",
      "Epoch 193/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 13.3813 - accuracy: 1.0000 - val_loss: 13.4944 - val_accuracy: 0.9524\n",
      "Epoch 194/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 13.3565 - accuracy: 1.0000 - val_loss: 13.4693 - val_accuracy: 0.9524\n",
      "Epoch 195/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.3298 - accuracy: 1.0000 - val_loss: 13.4438 - val_accuracy: 0.9524\n",
      "Epoch 196/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 13.3060 - accuracy: 1.0000 - val_loss: 13.4176 - val_accuracy: 0.9524\n",
      "Epoch 197/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.2826 - accuracy: 1.0000 - val_loss: 13.3913 - val_accuracy: 0.9524\n",
      "Epoch 198/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 13.2637 - accuracy: 1.0000 - val_loss: 13.3666 - val_accuracy: 0.9524\n",
      "Epoch 199/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.2407 - accuracy: 1.0000 - val_loss: 13.3449 - val_accuracy: 0.9524\n",
      "Epoch 200/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.2050 - accuracy: 1.0000 - val_loss: 13.3206 - val_accuracy: 0.9524\n",
      "Epoch 201/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.1822 - accuracy: 1.0000 - val_loss: 13.2963 - val_accuracy: 0.9524\n",
      "Epoch 202/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.1581 - accuracy: 1.0000 - val_loss: 13.2702 - val_accuracy: 0.9643\n",
      "Epoch 203/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 13.1307 - accuracy: 1.0000 - val_loss: 13.2447 - val_accuracy: 0.9643\n",
      "Epoch 204/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.1085 - accuracy: 1.0000 - val_loss: 13.2180 - val_accuracy: 0.9643\n",
      "Epoch 205/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.0805 - accuracy: 1.0000 - val_loss: 13.1911 - val_accuracy: 0.9643\n",
      "Epoch 206/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 13.0677 - accuracy: 0.9970 - val_loss: 13.1649 - val_accuracy: 0.9524\n",
      "Epoch 207/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.0361 - accuracy: 1.0000 - val_loss: 13.1465 - val_accuracy: 0.9524\n",
      "Epoch 208/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 13.0076 - accuracy: 1.0000 - val_loss: 13.1222 - val_accuracy: 0.9524\n",
      "Epoch 209/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.9790 - accuracy: 1.0000 - val_loss: 13.0979 - val_accuracy: 0.9524\n",
      "Epoch 210/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.9528 - accuracy: 1.0000 - val_loss: 13.0718 - val_accuracy: 0.9524\n",
      "Epoch 211/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.9373 - accuracy: 1.0000 - val_loss: 13.0451 - val_accuracy: 0.9643\n",
      "Epoch 212/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.9031 - accuracy: 1.0000 - val_loss: 13.0169 - val_accuracy: 0.9524\n",
      "Epoch 213/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.8776 - accuracy: 1.0000 - val_loss: 12.9910 - val_accuracy: 0.9524\n",
      "Epoch 214/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.8528 - accuracy: 1.0000 - val_loss: 12.9664 - val_accuracy: 0.9524\n",
      "Epoch 215/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.8306 - accuracy: 0.9940 - val_loss: 12.9406 - val_accuracy: 0.9524\n",
      "Epoch 216/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 12.8006 - accuracy: 1.0000 - val_loss: 12.9220 - val_accuracy: 0.9643\n",
      "Epoch 217/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 12.7761 - accuracy: 1.0000 - val_loss: 12.8984 - val_accuracy: 0.9643\n",
      "Epoch 218/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.7568 - accuracy: 1.0000 - val_loss: 12.8751 - val_accuracy: 0.9643\n",
      "Epoch 219/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.7277 - accuracy: 1.0000 - val_loss: 12.8492 - val_accuracy: 0.9643\n",
      "Epoch 220/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 12.7013 - accuracy: 1.0000 - val_loss: 12.8207 - val_accuracy: 0.9643\n",
      "Epoch 221/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.6732 - accuracy: 1.0000 - val_loss: 12.7931 - val_accuracy: 0.9643\n",
      "Epoch 222/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.6533 - accuracy: 1.0000 - val_loss: 12.7671 - val_accuracy: 0.9643\n",
      "Epoch 223/300\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 12.6212 - accuracy: 1.0000 - val_loss: 12.7420 - val_accuracy: 0.9762\n",
      "Epoch 224/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.5973 - accuracy: 1.0000 - val_loss: 12.7147 - val_accuracy: 0.9762\n",
      "Epoch 225/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 12.5747 - accuracy: 1.0000 - val_loss: 12.6868 - val_accuracy: 0.9762\n",
      "Epoch 226/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 12.5534 - accuracy: 1.0000 - val_loss: 12.6589 - val_accuracy: 0.9643\n",
      "Epoch 227/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 12.5196 - accuracy: 1.0000 - val_loss: 12.6322 - val_accuracy: 0.9643\n",
      "Epoch 228/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 12.5006 - accuracy: 1.0000 - val_loss: 12.6066 - val_accuracy: 0.9524\n",
      "Epoch 229/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.4743 - accuracy: 1.0000 - val_loss: 12.5826 - val_accuracy: 0.9524\n",
      "Epoch 230/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.4394 - accuracy: 1.0000 - val_loss: 12.5555 - val_accuracy: 0.9524\n",
      "Epoch 231/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.4196 - accuracy: 1.0000 - val_loss: 12.5282 - val_accuracy: 0.9524\n",
      "Epoch 232/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.4001 - accuracy: 1.0000 - val_loss: 12.5045 - val_accuracy: 0.9524\n",
      "Epoch 233/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.3631 - accuracy: 1.0000 - val_loss: 12.4766 - val_accuracy: 0.9643\n",
      "Epoch 234/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.3396 - accuracy: 1.0000 - val_loss: 12.4504 - val_accuracy: 0.9643\n",
      "Epoch 235/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.3137 - accuracy: 1.0000 - val_loss: 12.4248 - val_accuracy: 0.9643\n",
      "Epoch 236/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.2869 - accuracy: 1.0000 - val_loss: 12.3969 - val_accuracy: 0.9643\n",
      "Epoch 237/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.2601 - accuracy: 1.0000 - val_loss: 12.3694 - val_accuracy: 0.9643\n",
      "Epoch 238/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 12.2350 - accuracy: 1.0000 - val_loss: 12.3409 - val_accuracy: 0.9643\n",
      "Epoch 239/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.2079 - accuracy: 1.0000 - val_loss: 12.3142 - val_accuracy: 0.9524\n",
      "Epoch 240/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.1806 - accuracy: 1.0000 - val_loss: 12.2887 - val_accuracy: 0.9524\n",
      "Epoch 241/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.1646 - accuracy: 1.0000 - val_loss: 12.2640 - val_accuracy: 0.9524\n",
      "Epoch 242/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 12.1292 - accuracy: 1.0000 - val_loss: 12.2384 - val_accuracy: 0.9524\n",
      "Epoch 243/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.0992 - accuracy: 1.0000 - val_loss: 12.2116 - val_accuracy: 0.9524\n",
      "Epoch 244/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.0827 - accuracy: 0.9970 - val_loss: 12.1844 - val_accuracy: 0.9524\n",
      "Epoch 245/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 12.0475 - accuracy: 1.0000 - val_loss: 12.1576 - val_accuracy: 0.9643\n",
      "Epoch 246/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 12.0314 - accuracy: 1.0000 - val_loss: 12.1316 - val_accuracy: 0.9643\n",
      "Epoch 247/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 11.9972 - accuracy: 1.0000 - val_loss: 12.1154 - val_accuracy: 0.9524\n",
      "Epoch 248/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 11.9707 - accuracy: 1.0000 - val_loss: 12.0955 - val_accuracy: 0.9524\n",
      "Epoch 249/300\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 11.9408 - accuracy: 1.0000 - val_loss: 12.0687 - val_accuracy: 0.9524\n",
      "Epoch 250/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 11.9211 - accuracy: 1.0000 - val_loss: 12.0428 - val_accuracy: 0.9524\n",
      "Epoch 251/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.8929 - accuracy: 1.0000 - val_loss: 12.0155 - val_accuracy: 0.9524\n",
      "Epoch 252/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.8657 - accuracy: 1.0000 - val_loss: 11.9892 - val_accuracy: 0.9524\n",
      "Epoch 253/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.8374 - accuracy: 1.0000 - val_loss: 11.9616 - val_accuracy: 0.9643\n",
      "Epoch 254/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.8189 - accuracy: 1.0000 - val_loss: 11.9329 - val_accuracy: 0.9643\n",
      "Epoch 255/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.8053 - accuracy: 0.9970 - val_loss: 11.9040 - val_accuracy: 0.9524\n",
      "Epoch 256/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.7724 - accuracy: 0.9970 - val_loss: 11.8795 - val_accuracy: 0.9524\n",
      "Epoch 257/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.7341 - accuracy: 1.0000 - val_loss: 11.8570 - val_accuracy: 0.9524\n",
      "Epoch 258/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 11.7051 - accuracy: 1.0000 - val_loss: 11.8307 - val_accuracy: 0.9524\n",
      "Epoch 259/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 11.6815 - accuracy: 1.0000 - val_loss: 11.8044 - val_accuracy: 0.9524\n",
      "Epoch 260/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.6534 - accuracy: 1.0000 - val_loss: 11.7767 - val_accuracy: 0.9524\n",
      "Epoch 261/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 11.6290 - accuracy: 1.0000 - val_loss: 11.7474 - val_accuracy: 0.9524\n",
      "Epoch 262/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.6037 - accuracy: 1.0000 - val_loss: 11.7205 - val_accuracy: 0.9524\n",
      "Epoch 263/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.5744 - accuracy: 1.0000 - val_loss: 11.6941 - val_accuracy: 0.9643\n",
      "Epoch 264/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.5463 - accuracy: 1.0000 - val_loss: 11.6698 - val_accuracy: 0.9643\n",
      "Epoch 265/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 11.5266 - accuracy: 1.0000 - val_loss: 11.6432 - val_accuracy: 0.9643\n",
      "Epoch 266/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.4965 - accuracy: 1.0000 - val_loss: 11.6196 - val_accuracy: 0.9643\n",
      "Epoch 267/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 11.4878 - accuracy: 0.9911 - val_loss: 11.5967 - val_accuracy: 0.9643\n",
      "Epoch 268/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.4457 - accuracy: 1.0000 - val_loss: 11.5722 - val_accuracy: 0.9643\n",
      "Epoch 269/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.4202 - accuracy: 1.0000 - val_loss: 11.5439 - val_accuracy: 0.9643\n",
      "Epoch 270/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 11.3904 - accuracy: 1.0000 - val_loss: 11.5131 - val_accuracy: 0.9643\n",
      "Epoch 271/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.3686 - accuracy: 1.0000 - val_loss: 11.4823 - val_accuracy: 0.9643\n",
      "Epoch 272/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.3359 - accuracy: 1.0000 - val_loss: 11.4558 - val_accuracy: 0.9643\n",
      "Epoch 273/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 11.3131 - accuracy: 1.0000 - val_loss: 11.4297 - val_accuracy: 0.9643\n",
      "Epoch 274/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.2817 - accuracy: 1.0000 - val_loss: 11.4028 - val_accuracy: 0.9643\n",
      "Epoch 275/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 11.2714 - accuracy: 1.0000 - val_loss: 11.3788 - val_accuracy: 0.9643\n",
      "Epoch 276/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.2336 - accuracy: 1.0000 - val_loss: 11.3533 - val_accuracy: 0.9643\n",
      "Epoch 277/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.2037 - accuracy: 1.0000 - val_loss: 11.3282 - val_accuracy: 0.9643\n",
      "Epoch 278/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.1771 - accuracy: 1.0000 - val_loss: 11.3008 - val_accuracy: 0.9643\n",
      "Epoch 279/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 11.1539 - accuracy: 1.0000 - val_loss: 11.2714 - val_accuracy: 0.9643\n",
      "Epoch 280/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.1265 - accuracy: 1.0000 - val_loss: 11.2441 - val_accuracy: 0.9643\n",
      "Epoch 281/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 11.0989 - accuracy: 1.0000 - val_loss: 11.2187 - val_accuracy: 0.9643\n",
      "Epoch 282/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 11.0700 - accuracy: 1.0000 - val_loss: 11.1898 - val_accuracy: 0.9524\n",
      "Epoch 283/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 11.0533 - accuracy: 1.0000 - val_loss: 11.1621 - val_accuracy: 0.9643\n",
      "Epoch 284/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 11.0192 - accuracy: 1.0000 - val_loss: 11.1333 - val_accuracy: 0.9762\n",
      "Epoch 285/300\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 10.9904 - accuracy: 1.0000 - val_loss: 11.1049 - val_accuracy: 0.9762\n",
      "Epoch 286/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 10.9698 - accuracy: 1.0000 - val_loss: 11.0792 - val_accuracy: 0.9762\n",
      "Epoch 287/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 10.9392 - accuracy: 1.0000 - val_loss: 11.0545 - val_accuracy: 0.9762\n",
      "Epoch 288/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 10.9113 - accuracy: 1.0000 - val_loss: 11.0275 - val_accuracy: 0.9762\n",
      "Epoch 289/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 10.8889 - accuracy: 1.0000 - val_loss: 11.0032 - val_accuracy: 0.9643\n",
      "Epoch 290/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 10.8592 - accuracy: 1.0000 - val_loss: 10.9813 - val_accuracy: 0.9643\n",
      "Epoch 291/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 10.8336 - accuracy: 1.0000 - val_loss: 10.9559 - val_accuracy: 0.9643\n",
      "Epoch 292/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 10.8056 - accuracy: 1.0000 - val_loss: 10.9293 - val_accuracy: 0.9524\n",
      "Epoch 293/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 10.7874 - accuracy: 1.0000 - val_loss: 10.9007 - val_accuracy: 0.9524\n",
      "Epoch 294/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 10.7499 - accuracy: 1.0000 - val_loss: 10.8747 - val_accuracy: 0.9524\n",
      "Epoch 295/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 10.7250 - accuracy: 1.0000 - val_loss: 10.8473 - val_accuracy: 0.9643\n",
      "Epoch 296/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 10.6952 - accuracy: 1.0000 - val_loss: 10.8195 - val_accuracy: 0.9643\n",
      "Epoch 297/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 10.6723 - accuracy: 1.0000 - val_loss: 10.7917 - val_accuracy: 0.9643\n",
      "Epoch 298/300\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 10.6440 - accuracy: 1.0000 - val_loss: 10.7641 - val_accuracy: 0.9643\n",
      "Epoch 299/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 10.6167 - accuracy: 1.0000 - val_loss: 10.7366 - val_accuracy: 0.9643\n",
      "Epoch 300/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 10.5884 - accuracy: 1.0000 - val_loss: 10.7135 - val_accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer with Batch Normalization and a slightly higher regularization\n",
    "model.add(Dense(1024, kernel_regularizer=l2(0.005), input_shape=(1024,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Hidden layers with Batch Normalization, increased neurons, and Dropout\n",
    "model.add(Dense(1024, kernel_regularizer=l2(0.005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, kernel_regularizer=l2(0.005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, kernel_regularizer=l2(0.005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, kernel_regularizer=l2(0.005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, kernel_regularizer=l2(0.005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.00001)  # Further reduced learning rate\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Checkpoint to save the best model\n",
    "checkpoint = ModelCheckpoint(\"final_best_model.h5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "training_results = model.fit(X_train, \n",
    "                             y_train, \n",
    "                             epochs=300, \n",
    "                             batch_size=32,\n",
    "                             validation_data=(X_val, y_val),\n",
    "                             callbacks=[checkpoint, early_stopping],\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c27a1226-1d5d-48b1-8d37-c13c43df1169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acuuracy : 98.41%\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "precision: 0.9825\n",
      "Recall:  0.9825\n",
      "f1-score:  0.9825\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"final_best_model.h5\")\n",
    "test_results = best_model.evaluate(X_test, y_test, verbose = 0 ) \n",
    "print(f'Test Acuuracy : {test_results[1]*100:.2f}%')\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.round(y_pred_probs)\n",
    "precision = precision_score(y_test , y_pred )\n",
    "recall = recall_score (y_test , y_pred )\n",
    "f1= f1_score ( y_test , y_pred )\n",
    "print(f'precision: {precision:.4f}')\n",
    "print(f'Recall: {recall : .4f}')\n",
    "print(f'f1-score: {f1 : .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18dd0c-ae76-4829-ae33-c99f58e811ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
